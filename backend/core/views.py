# File: core/views.py
import os
import json
from django.conf import settings
from django.core.cache import cache
from django.utils import timezone
from rest_framework import status
from rest_framework.response import Response
from rest_framework.views import APIView
from rest_framework_simplejwt.tokens import RefreshToken
import uuid
import firebase_admin
from firebase_admin import credentials, auth
from django.db import IntegrityError
import requests
import stripe
from rest_framework.exceptions import ValidationError
from rest_framework.permissions import IsAuthenticated
import logging

logger = logging.getLogger(__name__)

from .models import User, Alert, Payment, CryptoData, FavoriteCrypto
from .serializers import (
    RegisterSerializer, LoginSerializer, LoginWithTokenSerializer,
    UserSerializer, AlertSerializer, PaymentSerializer,
    CryptoDataSerializer, CryptoDataFreeSerializer, CryptoDataBasicSerializer,
    FavoriteCryptoSerializer
)

# --- Import the tasks ---
from .tasks import send_activation_email_task, send_login_token_email_task

# --- Firebase Admin SDK Initialization ---
private_key = os.environ.get("FIREBASE_PRIVATE_KEY", "").replace('\\n', '\n')

# Only initialize Firebase if private key is properly provided
if private_key and len(private_key.strip()) > 100:  # Valid private key should be much longer
    cred_dict = {
      "type": "service_account",
      "project_id": "file-sharing-app-c63a0",
      "private_key_id": "82461d4f111c13496741bef3173b76a65e9ad993",
      "private_key": private_key,
      "client_email": "firebase-adminsdk-5oc6t@file-sharing-app-c63a0.iam.gserviceaccount.com",
      "client_id": "114923044052820733108",
      "auth_uri": "https://accounts.google.com/o/oauth2/auth",
      "token_uri": "https://oauth2.googleapis.com/token",
      "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
      "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-5oc6t%40file-sharing-app-c63a0.iam.gserviceaccount.com",
      "universe_domain": "googleapis.com"
    }
    cred = credentials.Certificate(cred_dict)

    if not firebase_admin._apps:
        firebase_admin.initialize_app(cred)
else:
    pass  # Firebase not initialized - FIREBASE_PRIVATE_KEY not properly configured

stripe.api_key = settings.STRIPE_SECRET_KEY
stripe_price_ids = {
    'basic': os.environ.get('STRIPE_PRICE_ID_BASIC'),
    'enterprise': os.environ.get('STRIPE_PRICE_ID_ENTERPRISE'),
}

# Test user emails that can bypass authentication (for load testing)
TEST_USER_EMAILS = [
    'testuser1@volusignal.com', 'testuser2@volusignal.com', 'testuser3@volusignal.com',
    'testuser4@volusignal.com', 'testuser5@volusignal.com', 'testuser6@volusignal.com',
    'testuser7@volusignal.com', 'testuser8@volusignal.com', 'testuser9@volusignal.com',
    'testuser10@volusignal.com', 'testuser11@volusignal.com', 'testuser12@volusignal.com',
    'testuser13@volusignal.com', 'testuser14@volusignal.com', 'testuser15@volusignal.com',
]


class TestLoginView(APIView):
    """
    Test login endpoint for load testing purposes.
    Allows test users to login without email verification.
    Only works for predefined test email addresses.
    """
    authentication_classes = []
    permission_classes = []

    def post(self, request):
        email = request.data.get('email', '').lower().strip()
        
        if not email:
            return Response({'error': 'Email is required'}, status=status.HTTP_400_BAD_REQUEST)
        
        # Only allow test user emails
        if email not in TEST_USER_EMAILS:
            logger.warning(f"Test login attempt with non-test email: {email}")
            return Response(
                {'error': 'Invalid test user email. Only authorized test accounts can use this endpoint.'},
                status=status.HTTP_403_FORBIDDEN
            )
        
        try:
            user = User.objects.get(email=email)
            
            # Generate JWT tokens
            refresh = RefreshToken.for_user(user)
            
            logger.info(f"Test user logged in: {email}")
            
            return Response({
                'refresh': str(refresh),
                'access': str(refresh.access_token),
                'first_name': user.first_name,
                'last_name': user.last_name,
                'email': user.email,
                'subscription_plan': user.subscription_plan,
                'is_premium_user': user.is_premium_user,
                'message': 'Test login successful'
            }, status=status.HTTP_200_OK)
            
        except User.DoesNotExist:
            return Response(
                {'error': f'Test user {email} not found. Run: python manage.py create_test_users'},
                status=status.HTTP_404_NOT_FOUND
            )


class RegisterView(APIView):
    def post(self, request):
        serializer = RegisterSerializer(data=request.data)
        if serializer.is_valid():
            user = serializer.save() 
            token = str(uuid.uuid4())
            user.activation_token = token
            user.save()
            send_activation_email_task.delay(user.email, user.first_name, token)
            return Response({'message': 'User registered successfully. An activation email has been sent.'}, status=status.HTTP_201_CREATED)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)

class RequestLoginTokenView(APIView):
    def post(self, request):
        serializer = LoginSerializer(data=request.data)
        if serializer.is_valid():
            email = serializer.validated_data['email']
            try:
                user = User.objects.get(email=email)
                if not user.is_active:
                    return Response({'error': 'Please activate your account first.'}, status=status.HTTP_403_FORBIDDEN)
                login_token = str(uuid.uuid4())
                user.login_token = login_token
                user.save()
                # Send email synchronously to avoid Celery worker queue delays
                send_login_token_email_task.apply(args=(email, user.first_name, login_token))
                return Response({'message': 'A login link has been sent to your email.'}, status=status.HTTP_200_OK)
            except User.DoesNotExist:
                return Response({'error': 'User with this email does not exist.'}, status=status.HTTP_404_NOT_FOUND)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)

class LoginWithTokenView(APIView):
    def post(self, request):
        serializer = LoginWithTokenSerializer(data=request.data)
        if serializer.is_valid():
            token = serializer.validated_data['token']
            try:
                user = User.objects.get(login_token=token, is_active=True)
                user.login_token = None
                user.save()
                refresh = RefreshToken.for_user(user)
                return Response({
                    'refresh': str(refresh),
                    'access': str(refresh.access_token),
                    'first_name': user.first_name,
                    'last_name': user.last_name,
                    'email': user.email,
                    'subscription_plan': user.subscription_plan,
                    'is_premium_user': user.is_premium_user,
                }, status=status.HTTP_200_OK)
            except User.DoesNotExist:
                return Response({'error': 'Invalid or expired login link.'}, status=status.HTTP_400_BAD_REQUEST)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)

class ActivateAccountView(APIView):
    def get(self, request, token):
        try:
            user = User.objects.get(activation_token=token, is_active=False)
            user.is_active = True
            user.activation_token = None
            user.save()
            return Response({'message': 'Account activated successfully.'}, status=status.HTTP_200_OK)
        except User.DoesNotExist:
            return Response({'message': 'Invalid activation link or account already activated.'}, status=status.HTTP_400_BAD_REQUEST)

class GoogleLoginView(APIView):
    authentication_classes = []
    permission_classes = []

    def post(self, request):
        try:
            # Enhanced logging for debugging
            logger.info(f"Google login attempt from IP: {request.META.get('REMOTE_ADDR')}")
            
            # Debug: Log the raw Authorization header
            auth_header = request.headers.get('Authorization', '')
            logger.info(f"Raw Authorization header: {auth_header[:50]}...")
            
            id_token = auth_header.split('Bearer ')[-1] if 'Bearer ' in auth_header else auth_header
            if not id_token or id_token == auth_header:
                logger.warning("Google login failed: No Authorization header or invalid format")
                return Response({'error': 'Authorization header missing or invalid format. Expected: Bearer <token>'}, status=status.HTTP_400_BAD_REQUEST)

            # Check if Firebase is properly initialized
            if not firebase_admin._apps:
                logger.error("Firebase not initialized - cannot verify ID token")
                return Response({'error': 'Authentication service not available.'}, status=status.HTTP_503_SERVICE_UNAVAILABLE)

            logger.info(f"Attempting to verify Firebase ID token (length: {len(id_token)}, starts with: {id_token[:20]}...)")
            
            # Add clock skew tolerance for Docker containers
            # This allows for small time differences between client and server
            decoded_token = auth.verify_id_token(id_token, clock_skew_seconds=60)
            
            email = decoded_token.get('email')
            first_name = request.data.get('first_name')
            last_name = request.data.get('last_name')

            if not email:
                logger.warning("Google login failed: No email in decoded token")
                return Response({'error': 'Email not found in Google token.'}, status=status.HTTP_400_BAD_REQUEST)

            try:
                user = User.objects.get(email=email)
                if not user.is_active:
                    user.is_active = True
                    user.save()
                logger.info(f"Existing user logged in: {email}")
            except User.DoesNotExist:
                user = User.objects.create_user(
                    username=email,
                    email=email,
                    first_name=first_name,
                    last_name=last_name,
                    is_active=True,
                    subscription_plan='free',
                    is_premium_user=False
                )
                logger.info(f"New user created: {email}")

            refresh = RefreshToken.for_user(user)
            logger.info(f"Google login successful for: {email}")
            return Response({
                'refresh': str(refresh),
                'access': str(refresh.access_token),
                'first_name': user.first_name,
                'last_name': user.last_name,
                'email': user.email,
                'subscription_plan': user.subscription_plan,
                'is_premium_user': user.is_premium_user,
            }, status=status.HTTP_200_OK)

        except auth.InvalidIdTokenError as e:
            logger.warning(f"Invalid Firebase ID token: {str(e)}")
            return Response({'error': 'Invalid Firebase ID token.', 'details': str(e)}, status=status.HTTP_401_UNAUTHORIZED)
        except Exception as e:
            logger.error(f"Error during Google login: {str(e)}")
            return Response({'error': 'Authentication failed.', 'details': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class FirebaseTestView(APIView):
    """Test endpoint to validate Firebase configuration"""
    authentication_classes = []
    permission_classes = []

    def get(self, request):
        try:
            firebase_initialized = bool(firebase_admin._apps)
            return Response({
                'firebase_initialized': firebase_initialized,
                'project_id': os.environ.get('FIREBASE_PROJECT_ID', 'Not set'),
                'client_email': os.environ.get('FIREBASE_CLIENT_EMAIL', 'Not set'),
                'private_key_set': bool(os.environ.get('FIREBASE_PRIVATE_KEY', '')),
                'apps_count': len(firebase_admin._apps) if firebase_admin._apps else 0
            })
        except Exception as e:
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class UserDetailView(APIView):
    permission_classes = [IsAuthenticated]
    def get(self, request):
        user = request.user
        serializer = UserSerializer(user)
        return Response(serializer.data)

class UserUpdateView(APIView):
    permission_classes = [IsAuthenticated]
    def patch(self, request):
        user = request.user
        serializer = UserSerializer(user, data=request.data, partial=True)
        if serializer.is_valid():
            serializer.save()
            return Response(serializer.data, status=status.HTTP_200_OK)
        # Return more detailed error messages
        error_message = 'Failed to update profile.'
        if serializer.errors:
            # Extract the first error message for better UX
            for field, errors in serializer.errors.items():
                if errors:
                    error_message = f"{field}: {errors[0]}"
                    break
        return Response({'error': error_message, 'details': serializer.errors}, status=status.HTTP_400_BAD_REQUEST)

class UpgradePlanView(APIView):
    permission_classes = [IsAuthenticated]
    def post(self, request):
        user = request.user
        new_plan = request.data.get('plan')
        if not new_plan or new_plan not in ['basic', 'enterprise']:
            return Response({'error': 'Invalid plan specified.'}, status=status.HTTP_400_BAD_REQUEST)
        try:
            checkout_session = stripe.checkout.Session.create(
                line_items=[{'price': stripe_price_ids[new_plan], 'quantity': 1,}],
                mode='subscription',
                success_url=f"{settings.FRONTEND_URL}/dashboard?upgrade=success",
                cancel_url=f"{settings.FRONTEND_URL}/upgrade-plan?upgrade=canceled",
                customer_email=user.email,
                client_reference_id=str(user.id)
            )
            return Response({'checkout_url': checkout_session.url}, status=status.HTTP_200_OK)
        except Exception as e:
            return Response({'error': str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class StripeWebhookView(APIView):
    permission_classes = []
    authentication_classes = []
    def post(self, request):
        payload = request.body
        sig_header = request.META.get('HTTP_STRIPE_SIGNATURE')
        event = None
        
        logger.info(f"Webhook received from Stripe")
        
        try:
            event = stripe.Webhook.construct_event(payload, sig_header, settings.STRIPE_WEBHOOK_SECRET)
            logger.info(f"Webhook signature verified. Event type: {event['type']}")
        except ValueError as e:
            logger.error(f"Webhook error - Invalid payload: {str(e)}")
            return Response(status=status.HTTP_400_BAD_REQUEST)
        except stripe.error.SignatureVerificationError as e:
            logger.error(f"Webhook error - Signature verification failed: {str(e)}")
            return Response(status=status.HTTP_400_BAD_REQUEST)

        if event['type'] == 'checkout.session.completed':
            session = event['data']['object']
            client_reference_id = session.get('client_reference_id')
            logger.info(f"Checkout session completed. Client ref ID: {client_reference_id}, Session ID: {session.id}")
            
            if client_reference_id:
                try:
                    from django.utils import timezone
                    from datetime import timedelta
                    
                    user = User.objects.get(id=client_reference_id)
                    logger.info(f"Found user: {user.email} (ID: {user.id})")
                    
                    line_items = stripe.checkout.Session.list_line_items(session.id, limit=1)
                    price_id = line_items['data'][0]['price']['id']
                    logger.info(f"Price ID from checkout: {price_id}")
                    
                    plan_map = {v: k for k, v in stripe_price_ids.items()}
                    new_plan = plan_map.get(price_id)
                    
                    if new_plan:
                        logger.info(f"Updating user {user.email} to plan: {new_plan}")
                        user.subscription_plan = new_plan
                        user.is_premium_user = True
                        user.stripe_customer_id = session.get('customer')
                        
                        # Set plan dates (30 days subscription)
                        user.plan_start_date = timezone.now()
                        user.plan_end_date = timezone.now() + timedelta(days=30)
                        
                        user.save()
                        logger.info(f"âœ… User {user.email} successfully upgraded to {new_plan}")
                        
                        Payment.objects.create(
                            user=user, stripe_charge_id=session.id,
                            amount=session.amount_total, status=session.payment_status,
                            plan=new_plan
                        )
                        logger.info(f"Payment record created for user {user.email}")
                    else:
                        logger.warning(f"Price ID {price_id} not found in plan mapping")
                except User.DoesNotExist:
                    logger.error(f"User with ID {client_reference_id} not found")
                except Exception as e:
                    logger.error(f"Error processing webhook: {str(e)}", exc_info=True)
            else:
                logger.warning("No client_reference_id in checkout session")
        else:
            logger.info(f"Webhook event type {event['type']} - no action needed")
            
        return Response(status=status.HTTP_200_OK)

class PaymentHistoryView(APIView):
    permission_classes = [IsAuthenticated]
    def get(self, request):
        payments = Payment.objects.filter(user=request.user)
        serializer = PaymentSerializer(payments, many=True)
        return Response(serializer.data)

class AlertsView(APIView):
    permission_classes = [IsAuthenticated]
    def get(self, request):
        alerts = Alert.objects.filter(user=request.user)
        serializer = AlertSerializer(alerts, many=True)
        return Response(serializer.data)
    def post(self, request):
        # Restrict alert creation to premium plans
        user = request.user
        user_plan = getattr(user, 'subscription_plan', 'free') or 'free'
        is_premium = getattr(user, 'is_premium_user', False) or user_plan in ['basic', 'enterprise']
        if not is_premium:
            return Response({'error': 'Alerts are a premium feature. Please upgrade your plan to create alerts.'}, status=status.HTTP_403_FORBIDDEN)
        serializer = AlertSerializer(data=request.data)
        if serializer.is_valid():
            serializer.save(user=request.user)
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
    def delete(self, request, alert_id):
        try:
            alert = Alert.objects.get(id=alert_id, user=request.user)
            alert.delete()
            return Response(status=status.HTTP_204_NO_CONTENT)
        except Alert.DoesNotExist:
            return Response({'error': 'Alert not found.'}, status=status.HTTP_404_NOT_FOUND)

class BinanceDataView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        try:
            user = request.user
            
            # Pagination parameters for handling ALL symbols
            page = int(request.GET.get('page', 1))
            page_size = min(int(request.GET.get('page_size', 100)), 500)  # Allow up to 500 symbols
            offset = (page - 1) * page_size
            
            # Base currency filter - supports USDT, USDC, FDUSD, BNB, BTC
            base_currency = request.GET.get('base_currency', 'USDT').upper()
            valid_currencies = ['USDT', 'USDC', 'FDUSD', 'BNB', 'BTC']
            if base_currency not in valid_currencies:
                base_currency = 'USDT'  # Default to USDT if invalid
            
            # Sorting parameters
            sort_by = request.GET.get('sort_by', 'profit')  # profit, volume, latest, price
            sort_order = request.GET.get('sort_order', 'desc')  # asc, desc
            
            # Build cache key with sorting and base currency
            cache_key = f'crypto_data_{user.subscription_plan}_{base_currency}_page_{page}_size_{page_size}_sort_{sort_by}_{sort_order}'
            cached_data = cache.get(cache_key)
            
            if cached_data is None:
                # Get total count for pagination - supports ALL currencies
                total_count = CryptoData.objects.filter(
                    symbol__endswith=base_currency,
                    last_price__isnull=False,
                    quote_volume_24h__gt=0
                ).count()
                
                # Determine sorting field
                sort_field = '-price_change_percent_24h'  # Default: Most profitable first
                if sort_by == 'volume':
                    sort_field = '-quote_volume_24h'
                elif sort_by == 'latest':
                    sort_field = '-id'  # Latest added symbols
                elif sort_by == 'price':
                    sort_field = '-last_price'
                elif sort_by == 'profit':
                    sort_field = '-price_change_percent_24h'
                
                # Apply sort order
                if sort_order == 'asc':
                    sort_field = sort_field.lstrip('-')
                
                # Get fresh data from database with pagination and sorting
                # Supports ALL currencies based on base_currency parameter
                crypto_data = CryptoData.objects.filter(
                    symbol__endswith=base_currency,
                    last_price__isnull=False,
                    quote_volume_24h__gt=0
                ).order_by(sort_field)[offset:offset + page_size]
                
                # Serialize based on user plan (no symbol count cap for any plan)
                if user.subscription_plan == 'enterprise':
                    serializer = CryptoDataSerializer(crypto_data, many=True)
                elif user.subscription_plan == 'basic':
                    serializer = CryptoDataBasicSerializer(crypto_data, many=True)
                else:  # free
                    serializer = CryptoDataFreeSerializer(crypto_data, many=True)
                
                # Prepare response with pagination info
                cached_data = {
                    'data': serializer.data,
                    'plan': user.subscription_plan,
                    'is_premium': user.is_premium_user or user.subscription_plan in ['basic', 'enterprise'],
                    'base_currency': base_currency,  # Include selected base currency
                    'pagination': {
                        'current_page': page,
                        'page_size': page_size,
                        'total_count': total_count,
                        'total_pages': (total_count + page_size - 1) // page_size,
                        'has_next': offset + page_size < total_count,
                        'has_previous': page > 1
                    },
                    'sorting': {
                        'sort_by': sort_by,
                        'sort_order': sort_order,
                        'available_sorts': ['profit', 'volume', 'latest', 'price']
                    },
                    'last_updated': timezone.now().isoformat(),
                    'symbols_in_page': len(serializer.data)
                }
                
                # Cache for 30 seconds for real-time updates
                cache.set(cache_key, cached_data, 30)

            return Response(cached_data, status=status.HTTP_200_OK)

        except Exception as e:
            logger.error(f"Error in BinanceDataView: {e}")
            return Response({"error": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

class FavoriteCryptoView(APIView):
    permission_classes = [IsAuthenticated]

    def get(self, request):
        favorites = FavoriteCrypto.objects.filter(user=request.user)
        serializer = FavoriteCryptoSerializer(favorites, many=True)
        return Response(serializer.data, status=status.HTTP_200_OK)

    def post(self, request):
        symbol = request.data.get('symbol')
        if not symbol:
            return Response({'error': 'Symbol is required.'}, status=status.HTTP_400_BAD_REQUEST)

        favorite, created = FavoriteCrypto.objects.get_or_create(user=request.user, symbol=symbol)

        if created:
            serializer = FavoriteCryptoSerializer(favorite)
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        else:
            return Response({'message': 'Symbol already in favorites.'}, status=status.HTTP_200_OK)

    def delete(self, request):
        symbol = request.data.get('symbol')
        if not symbol:
            return Response({'error': 'Symbol is required.'}, status=status.HTTP_400_BAD_REQUEST)

        try:
            favorite = FavoriteCrypto.objects.get(user=request.user, symbol=symbol)
            favorite.delete()
            return Response(status=status.HTTP_204_NO_CONTENT)
        except FavoriteCrypto.DoesNotExist:
            return Response({'error': 'Favorite not found.'}, status=status.HTTP_404_NOT_FOUND)

class HealthCheckView(APIView):
    """
    Health check endpoint for load balancer and monitoring
    """
    authentication_classes = []
    permission_classes = []

    def get(self, request):
        try:
            # Check database connectivity
            from django.db import connection
            cursor = connection.cursor()
            cursor.execute("SELECT 1")
            
            # Check Redis connectivity
            from django.core.cache import cache
            cache.set('health_check', 'ok', 10)
            cache_status = cache.get('health_check')
            
            # Check if we have crypto data
            # Health check data
            crypto_count = CryptoData.objects.filter(
                symbol__endswith='USDT'
            ).count()
            
            health_data = {
                'status': 'healthy',
                'timestamp': settings.TIME_ZONE,
                'database': 'connected',
                'cache': 'connected' if cache_status == 'ok' else 'disconnected',
                'crypto_symbols': crypto_count,
                'version': '1.0.0'
            }
            
            return Response(health_data, status=status.HTTP_200_OK)
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return Response({
                'status': 'unhealthy',
                'error': str(e)
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)

class MetricsView(APIView):
    """
    Metrics endpoint for monitoring and observability
    """
    authentication_classes = []
    permission_classes = []

    def get(self, request):
        try:
            from django.db import connection
            
            # Database metrics
            with connection.cursor() as cursor:
                # Total symbols
                cursor.execute("SELECT COUNT(*) FROM core_cryptodata")
                total_symbols = cursor.fetchone()[0]
                
                # Active users count
                cursor.execute("SELECT COUNT(*) FROM core_user WHERE is_active = true")
                active_users = cursor.fetchone()[0]
                
                # Premium users count
                cursor.execute("SELECT COUNT(*) FROM core_user WHERE is_premium_user = true")
                premium_users = cursor.fetchone()[0]
                
                # Recent alerts count
                cursor.execute("""
                    SELECT COUNT(*) FROM core_alert 
                    WHERE created_at > NOW() - INTERVAL '24 hours'
                """)
                recent_alerts = cursor.fetchone()[0]
            
            # Cache metrics
            cache_info = {
                'redis_connected': True
            }
            
            try:
                cache.set('metrics_test', 'ok', 5)
                cache.get('metrics_test')
            except Exception:
                cache_info['redis_connected'] = False
            
            metrics = {
                'timestamp': timezone.now().isoformat(),
                'database': {
                    'total_crypto_symbols': total_symbols,
                    'active_users': active_users,
                    'premium_users': premium_users,
                    'recent_alerts_24h': recent_alerts,
                },
                'cache': cache_info,
                'application': {
                    'version': '1.0.0',
                    'environment': 'development' if settings.DEBUG else 'production',
                }
            }
            
            return Response(metrics, status=status.HTTP_200_OK)
            
        except Exception as e:
            logger.error(f"Metrics collection failed: {e}")
            return Response({
                'error': 'Failed to collect metrics',
                'details': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class CoinSymbolsView(APIView):
    """
    API endpoint to fetch all available coin symbols from the database.
    Returns a list of unique symbols for use in dropdowns and search.
    """
    permission_classes = [IsAuthenticated]
    
    def get(self, request):
        try:
            # Get all symbols from the database, ordered alphabetically
            # Using values_list to get only the symbol field for performance
            symbols = CryptoData.objects.filter(
                symbol__isnull=False
            ).values_list('symbol', flat=True).order_by('symbol')
            
            # Convert QuerySet to list
            symbol_list = list(symbols)
            
            return Response({
                'symbols': symbol_list,
                'count': len(symbol_list)
            }, status=status.HTTP_200_OK)
            
        except Exception as e:
            logger.error(f"Failed to fetch coin symbols: {e}")
            return Response({
                'error': 'Failed to fetch coin symbols',
                'details': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)


class ManualRefreshView(APIView):
    """
    API endpoint to manually trigger data refresh from Binance
    Fetches REAL LIVE data directly from Binance API
    - FREE users: Only basic data (no calculated columns to prevent data leakage)
    - PAID users (basic/enterprise): Full data with all calculated columns
    """
    permission_classes = [IsAuthenticated]
    
    def post(self, request):
        try:
            import requests
            from decimal import Decimal
            import concurrent.futures
            
            logger.info(f"Manual refresh triggered by user {request.user.email}")
            
            # Check user's subscription plan
            user_plan = getattr(request.user, 'subscription_plan', 'free') or 'free'
            is_paid_user = user_plan in ['basic', 'enterprise']
            
            logger.info(f"User {request.user.email} plan: {user_plan}, is_paid: {is_paid_user}")
            
            # Get base currency from request (default USDT)
            base_currency = request.data.get('base_currency', 'USDT').upper()
            page_size = min(int(request.data.get('page_size', 100)), 500)
            
            # Step 1: Fetch exchangeInfo to get list of ACTIVE trading pairs only
            exchange_info_response = requests.get('https://api.binance.com/api/v3/exchangeInfo', timeout=10)
            exchange_info_response.raise_for_status()
            exchange_info = exchange_info_response.json()
            
            # Build set of actively trading symbols for the requested quote currency
            active_symbols = set()
            for symbol_info in exchange_info.get('symbols', []):
                if (symbol_info.get('status') == 'TRADING' and 
                    symbol_info.get('quoteAsset') == base_currency):
                    active_symbols.add(symbol_info['symbol'])
            
            logger.info(f"Found {len(active_symbols)} active trading pairs for {base_currency}")
            
            # Step 2: Fetch 24hr ticker data (fast - single API call)
            response = requests.get('https://api.binance.com/api/v3/ticker/24hr', timeout=10)
            response.raise_for_status()
            binance_data = response.json()
            
            # Filter: only active symbols with volume > 0
            filtered_data = []
            for item in binance_data:
                symbol = item.get('symbol', '')
                # IMPORTANT: Only include ACTIVELY TRADING symbols
                if symbol not in active_symbols:
                    continue
                quote_volume = float(item.get('quoteVolume', 0))
                if quote_volume <= 0:
                    continue
                filtered_data.append(item)
            
            # Sort by 24h price change (most profitable first)
            filtered_data.sort(key=lambda x: float(x.get('priceChangePercent', 0)), reverse=True)
            top_symbols = filtered_data[:page_size]
            
            # FREE USERS: Return only basic data (no calculated columns)
            # IMPORTANT: Convert string values to proper numeric types for frontend sorting
            if not is_paid_user:
                live_data = []
                for item in top_symbols:
                    live_data.append({
                        'symbol': item['symbol'],
                        'last_price': float(item['lastPrice']),
                        'price_change_percent_24h': float(item['priceChangePercent']),
                        'high_price_24h': float(item['highPrice']),
                        'low_price_24h': float(item['lowPrice']),
                        'quote_volume_24h': float(item['quoteVolume']),
                        'bid_price': float(item.get('bidPrice') or 0),
                        'ask_price': float(item.get('askPrice') or 0),
                    })
                
                logger.info(f"Manual refresh complete (FREE user): {len(live_data)} symbols with basic data for {base_currency}")
                
                return Response({
                    'status': 'success',
                    'message': f'Live data fetched from Binance.',
                    'data': live_data,
                    'symbols_updated': len(live_data),
                    'base_currency': base_currency,
                    'last_updated': timezone.now().isoformat()
                }, status=status.HTTP_200_OK)
            
            # PAID USERS: Fetch klines for calculated columns
            def fetch_klines_for_symbol(ticker_item):
                """Fetch klines and calculate metrics for a single symbol"""
                symbol = ticker_item['symbol']
                current_price = float(ticker_item['lastPrice'])
                
                try:
                    # Fetch 1-minute klines (last 65 candles - need 61+ for 60m calculations)
                    klines_url = f"https://api.binance.com/api/v3/klines?symbol={symbol}&interval=1m&limit=65"
                    klines_response = requests.get(klines_url, timeout=5)
                    
                    if klines_response.status_code != 200:
                        return self._basic_data(ticker_item)
                    
                    klines = klines_response.json()
                    if len(klines) < 2:
                        return self._basic_data(ticker_item)
                    
                    # Build metrics with calculated columns
                    # IMPORTANT: All values as NUMBERS for proper frontend sorting
                    metrics = {
                        'symbol': symbol,
                        'last_price': float(ticker_item['lastPrice']),
                        'price_change_percent_24h': float(ticker_item['priceChangePercent']),
                        'high_price_24h': float(ticker_item['highPrice']),
                        'low_price_24h': float(ticker_item['lowPrice']),
                        'quote_volume_24h': float(ticker_item['quoteVolume']),
                        'bid_price': float(ticker_item.get('bidPrice') or 0),
                        'ask_price': float(ticker_item.get('askPrice') or 0),
                    }
                    
                    # Calculate spread (handle zero bid price)
                    bid = float(ticker_item.get('bidPrice') or 0)
                    ask = float(ticker_item.get('askPrice') or 0)
                    if bid > 0 and ask > 0:
                        metrics['spread'] = round(ask - bid, 10)
                    else:
                        metrics['spread'] = 0
                    
                    # Helper function to calculate RSI
                    def calculate_rsi(closes, period=14):
                        """Calculate RSI from closing prices"""
                        if len(closes) < period + 1:
                            return None
                        
                        gains = []
                        losses = []
                        for i in range(1, len(closes)):
                            change = closes[i] - closes[i-1]
                            if change > 0:
                                gains.append(change)
                                losses.append(0)
                            else:
                                gains.append(0)
                                losses.append(abs(change))
                        
                        if len(gains) < period:
                            return None
                        
                        # Use simple moving average for first RSI
                        avg_gain = sum(gains[-period:]) / period
                        avg_loss = sum(losses[-period:]) / period
                        
                        if avg_loss == 0:
                            return 100.0
                        
                        rs = avg_gain / avg_loss
                        rsi = 100 - (100 / (1 + rs))
                        return round(rsi, 2)
                    
                    # Get closing prices for RSI calculation
                    closes = [float(k[4]) for k in klines]
                    
                    # Calculate RSI for different periods
                    # RSI 1m (use last 15 candles)
                    if len(closes) >= 15:
                        rsi_1m = calculate_rsi(closes[-15:], 14)
                        if rsi_1m is not None:
                            metrics['rsi_1m'] = rsi_1m
                    
                    # RSI 3m (use last 17 candles)
                    if len(closes) >= 17:
                        rsi_3m = calculate_rsi(closes[-17:], 14)
                        if rsi_3m is not None:
                            metrics['rsi_3m'] = rsi_3m
                    
                    # RSI 5m (use last 19 candles)
                    if len(closes) >= 19:
                        rsi_5m = calculate_rsi(closes[-19:], 14)
                        if rsi_5m is not None:
                            metrics['rsi_5m'] = rsi_5m
                    
                    # RSI 15m (use last 29 candles)
                    if len(closes) >= 29:
                        rsi_15m = calculate_rsi(closes[-29:], 14)
                        if rsi_15m is not None:
                            metrics['rsi_15m'] = rsi_15m
                    
                    # Calculate REAL price changes from klines
                    # ALL VALUES AS NUMBERS for proper frontend sorting
                    # 1 minute ago (index -2 because -1 is current incomplete candle)
                    if len(klines) >= 2:
                        m1_price = float(klines[-2][4])  # Close price
                        m1_volume = float(klines[-2][7])  # Quote volume
                        metrics['m1'] = round(((current_price - m1_price) / m1_price) * 100, 4) if m1_price > 0 else 0
                        metrics['m1_r_pct'] = metrics['m1']
                        metrics['m1_vol'] = round(m1_volume, 2)
                        metrics['m1_low'] = float(klines[-2][3])
                        metrics['m1_high'] = float(klines[-2][2])
                        m1_low = float(klines[-2][3])
                        m1_high = float(klines[-2][2])
                        metrics['m1_range_pct'] = round(((m1_high - m1_low) / m1_low) * 100, 4) if m1_low > 0 else 0
                    
                    # 2 minutes ago
                    if len(klines) >= 3:
                        m2_price = float(klines[-3][4])
                        m2_volume = sum(float(klines[i][7]) for i in range(-2, 0))
                        metrics['m2'] = round(((current_price - m2_price) / m2_price) * 100, 4) if m2_price > 0 else 0
                        metrics['m2_r_pct'] = metrics['m2']
                        metrics['m2_vol'] = round(m2_volume, 2)
                        m2_highs = [float(klines[i][2]) for i in range(-2, 0)]
                        m2_lows = [float(klines[i][3]) for i in range(-2, 0)]
                        metrics['m2_low'] = min(m2_lows)
                        metrics['m2_high'] = max(m2_highs)
                        metrics['m2_range_pct'] = round(((max(m2_highs) - min(m2_lows)) / min(m2_lows)) * 100, 4) if min(m2_lows) > 0 else 0
                    
                    # 3 minutes ago
                    if len(klines) >= 4:
                        m3_price = float(klines[-4][4])
                        m3_volume = sum(float(klines[i][7]) for i in range(-3, 0))
                        metrics['m3'] = round(((current_price - m3_price) / m3_price) * 100, 4) if m3_price > 0 else 0
                        metrics['m3_r_pct'] = metrics['m3']
                        metrics['m3_vol'] = round(m3_volume, 2)
                        m3_highs = [float(klines[i][2]) for i in range(-3, 0)]
                        m3_lows = [float(klines[i][3]) for i in range(-3, 0)]
                        metrics['m3_low'] = min(m3_lows)
                        metrics['m3_high'] = max(m3_highs)
                        metrics['m3_range_pct'] = round(((max(m3_highs) - min(m3_lows)) / min(m3_lows)) * 100, 4) if min(m3_lows) > 0 else 0
                    
                    # 5 minutes ago
                    if len(klines) >= 6:
                        m5_price = float(klines[-6][4])
                        m5_volume = sum(float(klines[i][7]) for i in range(-5, 0))
                        metrics['m5'] = round(((current_price - m5_price) / m5_price) * 100, 4) if m5_price > 0 else 0
                        metrics['m5_r_pct'] = metrics['m5']
                        metrics['m5_vol'] = round(m5_volume, 2)
                        m5_highs = [float(klines[i][2]) for i in range(-5, 0)]
                        m5_lows = [float(klines[i][3]) for i in range(-5, 0)]
                        metrics['m5_low'] = min(m5_lows)
                        metrics['m5_high'] = max(m5_highs)
                        metrics['m5_range_pct'] = round(((max(m5_highs) - min(m5_lows)) / min(m5_lows)) * 100, 4) if min(m5_lows) > 0 else 0
                    
                    # 10 minutes ago
                    if len(klines) >= 11:
                        m10_price = float(klines[-11][4])
                        m10_volume = sum(float(klines[i][7]) for i in range(-10, 0))
                        metrics['m10'] = round(((current_price - m10_price) / m10_price) * 100, 4) if m10_price > 0 else 0
                        metrics['m10_r_pct'] = metrics['m10']
                        metrics['m10_vol'] = round(m10_volume, 2)
                        m10_highs = [float(klines[i][2]) for i in range(-10, 0)]
                        m10_lows = [float(klines[i][3]) for i in range(-10, 0)]
                        metrics['m10_low'] = min(m10_lows)
                        metrics['m10_high'] = max(m10_highs)
                        metrics['m10_range_pct'] = round(((max(m10_highs) - min(m10_lows)) / min(m10_lows)) * 100, 4) if min(m10_lows) > 0 else 0
                    
                    # 15 minutes ago
                    if len(klines) >= 16:
                        m15_price = float(klines[-16][4])
                        m15_volume = sum(float(klines[i][7]) for i in range(-15, 0))
                        metrics['m15'] = round(((current_price - m15_price) / m15_price) * 100, 4) if m15_price > 0 else 0
                        metrics['m15_r_pct'] = metrics['m15']
                        metrics['m15_vol'] = round(m15_volume, 2)
                        m15_highs = [float(klines[i][2]) for i in range(-15, 0)]
                        m15_lows = [float(klines[i][3]) for i in range(-15, 0)]
                        metrics['m15_low'] = min(m15_lows)
                        metrics['m15_high'] = max(m15_highs)
                        metrics['m15_range_pct'] = round(((max(m15_highs) - min(m15_lows)) / min(m15_lows)) * 100, 4) if min(m15_lows) > 0 else 0
                    
                    # 60 minutes ago (1 hour) - need klines[-61] for 60 minutes ago price
                    if len(klines) >= 61:
                        m60_price = float(klines[-61][4])  # Price 60 minutes ago
                        m60_volume = sum(float(klines[i][7]) for i in range(-60, 0))
                        metrics['m60'] = round(((current_price - m60_price) / m60_price) * 100, 4) if m60_price > 0 else 0
                        metrics['m60_r_pct'] = metrics['m60']
                        metrics['m60_vol'] = round(m60_volume, 2)
                        m60_highs = [float(klines[i][2]) for i in range(-60, 0)]
                        m60_lows = [float(klines[i][3]) for i in range(-60, 0)]
                        metrics['m60_low'] = min(m60_lows)
                        metrics['m60_high'] = max(m60_highs)
                        metrics['m60_range_pct'] = round(((max(m60_highs) - min(m60_lows)) / min(m60_lows)) * 100, 4) if min(m60_lows) > 0 else 0
                    
                    # Calculate volume percentages
                    total_vol_24h = float(ticker_item['quoteVolume'])
                    if total_vol_24h > 0:
                        if 'm1_vol' in metrics:
                            metrics['m1_vol_pct'] = round((metrics['m1_vol'] / total_vol_24h) * 100, 4)
                        if 'm2_vol' in metrics:
                            metrics['m2_vol_pct'] = round((metrics['m2_vol'] / total_vol_24h) * 100, 4)
                        if 'm3_vol' in metrics:
                            metrics['m3_vol_pct'] = round((metrics['m3_vol'] / total_vol_24h) * 100, 4)
                        if 'm5_vol' in metrics:
                            metrics['m5_vol_pct'] = round((metrics['m5_vol'] / total_vol_24h) * 100, 4)
                        if 'm10_vol' in metrics:
                            metrics['m10_vol_pct'] = round((metrics['m10_vol'] / total_vol_24h) * 100, 4)
                        if 'm15_vol' in metrics:
                            metrics['m15_vol_pct'] = round((metrics['m15_vol'] / total_vol_24h) * 100, 4)
                        if 'm60_vol' in metrics:
                            metrics['m60_vol_pct'] = round((metrics['m60_vol'] / total_vol_24h) * 100, 4)
                    
                    # Calculate buy/sell volumes from taker buy volume
                    for tf, count in [('m1', 1), ('m2', 2), ('m3', 3), ('m5', 5), ('m10', 10), ('m15', 15)]:
                        if len(klines) >= count + 1:
                            total_vol = sum(float(klines[j][7]) for j in range(-count, 0))
                            buy_vol = sum(float(klines[j][10]) for j in range(-count, 0))
                            sell_vol = total_vol - buy_vol
                            metrics[f'{tf}_bv'] = round(buy_vol, 2)
                            metrics[f'{tf}_sv'] = round(sell_vol, 2)
                            metrics[f'{tf}_nv'] = round(buy_vol - sell_vol, 2)
                    
                    # 60-minute buy/sell volumes (need 61 candles)
                    if len(klines) >= 61:
                        total_vol = sum(float(klines[j][7]) for j in range(-60, 0))
                        buy_vol = sum(float(klines[j][10]) for j in range(-60, 0))
                        sell_vol = total_vol - buy_vol
                        metrics['m60_bv'] = round(buy_vol, 2)
                        metrics['m60_sv'] = round(sell_vol, 2)
                        metrics['m60_nv'] = round(buy_vol - sell_vol, 2)
                    
                    return metrics
                    
                except Exception as e:
                    logger.warning(f"Failed to fetch klines for {symbol}: {e}")
                    return self._basic_data(ticker_item)
            
            # Use ThreadPoolExecutor for parallel klines fetching (much faster!)
            live_data = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                futures = {executor.submit(fetch_klines_for_symbol, item): item for item in top_symbols}
                for future in concurrent.futures.as_completed(futures):
                    try:
                        result = future.result()
                        if result:
                            live_data.append(result)
                    except Exception as e:
                        logger.error(f"Error in parallel fetch: {e}")
            
            # Sort by price change again (parallel execution may change order)
            live_data.sort(key=lambda x: float(x.get('price_change_percent_24h', 0)), reverse=True)
            
            logger.info(f"Manual refresh complete (PAID user): {len(live_data)} symbols with calculated data for {base_currency}")
            
            return Response({
                'status': 'success',
                'message': f'Live data fetched from Binance with real calculations.',
                'data': live_data,
                'symbols_updated': len(live_data),
                'base_currency': base_currency,
                'last_updated': timezone.now().isoformat()
            }, status=status.HTTP_200_OK)
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Binance API error: {e}")
            return Response({
                'error': 'Failed to fetch data from Binance',
                'details': str(e)
            }, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        except Exception as e:
            logger.error(f"Failed to trigger manual refresh: {e}")
            return Response({
                'error': 'Failed to refresh data',
                'details': str(e)
            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
    
    def _basic_data(self, ticker_item):
        """Return basic data without klines calculations - ALL NUMERIC VALUES"""
        return {
            'symbol': ticker_item['symbol'],
            'last_price': float(ticker_item['lastPrice']),
            'price_change_percent_24h': float(ticker_item['priceChangePercent']),
            'high_price_24h': float(ticker_item['highPrice']),
            'low_price_24h': float(ticker_item['lowPrice']),
            'quote_volume_24h': float(ticker_item['quoteVolume']),
            'bid_price': float(ticker_item['bidPrice']) if ticker_item.get('bidPrice') else None,
            'ask_price': float(ticker_item['askPrice']) if ticker_item.get('askPrice') else None,
        }
